{% extends "base.html" %}

{% block title %}Langchain Interview Test{% endblock %}

{% block styles %}
<style>
  .interview-container {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
  }
  
  .prompt-section {
    margin-bottom: 30px;
    padding: 15px;
    border: 1px solid #ddd;
    border-radius: 5px;
    background-color: #f9f9f9;
  }
  
  .interview-section {
    display: flex;
    flex-direction: column;
    gap: 20px;
  }
  
  .transcript-area {
    height: 300px;
    overflow-y: auto;
    padding: 15px;
    border: 1px solid #ddd;
    border-radius: 5px;
    background-color: #fff;
    margin-bottom: 20px;
  }
  
  .message {
    margin-bottom: 10px;
    padding: 10px;
    border-radius: 5px;
  }
  
  .message.assistant {
    background-color: #f0f7ff;
    border-left: 3px solid #1890ff;
  }
  
  .message.user {
    background-color: #f6f6f6;
    border-left: 3px solid #999;
  }
  
  .controls {
    display: flex;
    flex-direction: column;
    gap: 15px;
  }
  
  .response-area {
    display: flex;
    gap: 10px;
  }
  
  .response-area textarea {
    flex: 1;
    min-height: 80px;
    padding: 10px;
    border: 1px solid #ddd;
    border-radius: 5px;
  }
  
  .audio-controls {
    display: flex;
    gap: 10px;
    margin-top: 10px;
  }
  
  .action-buttons {
    display: flex;
    gap: 10px;
  }
  
  button {
    padding: 8px 15px;
    border: none;
    border-radius: 5px;
    cursor: pointer;
    font-weight: bold;
  }
  
  button.primary {
    background-color: #1890ff;
    color: white;
  }
  
  button.secondary {
    background-color: #f0f0f0;
    color: #333;
  }
  
  button.danger {
    background-color: #ff4d4f;
    color: white;
  }
  
  button.success {
    background-color: #52c41a;
    color: white;
  }
  
  button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
  }
  
  .hidden {
    display: none;
  }
  
  .recording {
    color: #ff4d4f;
    animation: pulse 1.5s infinite;
  }
  
  @keyframes pulse {
    0% { opacity: 1; }
    50% { opacity: 0.5; }
    100% { opacity: 1; }
  }
  
  .settings {
    margin-bottom: 20px;
    padding: 15px;
    border: 1px solid #ddd;
    border-radius: 5px;
  }
  
  .settings select {
    padding: 8px;
    border-radius: 5px;
    border: 1px solid #ddd;
  }
  
  .analysis-section {
    margin-top: 30px;
    padding: 15px;
    border: 1px solid #ddd;
    border-radius: 5px;
    background-color: #fffbe6;
  }
</style>
{% endblock %}

{% block content %}
<div class="interview-container">
  <h1>Langchain Interview Test</h1>
  <p>This page allows you to test the Langchain voice interview functionality.</p>
  
  <!-- Settings -->
  <div class="settings">
    <h3>Voice Settings</h3>
    <label for="voiceSelect">Select AI Voice: </label>
    <select id="voiceSelect">
      {% for voice in voices %}
      <option value="{{ voice.id }}">{{ voice.name }}</option>
      {% endfor %}
    </select>
    
    <div style="margin-top: 15px;">
      <button id="testMicButton" class="secondary">
        <i class="fas fa-microphone"></i> Test Microphone
      </button>
      <span id="micTestResult" style="margin-left: 10px;"></span>
    </div>
    
    <div style="margin-top: 10px; font-size: 0.9em; color: #666; background-color: #f5f5f5; padding: 10px; border-radius: 5px;">
      <p><strong>Voice Recording Tips:</strong></p>
      <ul style="margin-top: 5px; padding-left: 20px;">
        <li>Test your microphone before starting the interview</li>
        <li>Speak clearly and at a normal pace when recording</li>
        <li>If transcription fails, you can type your response manually</li>
        <li>Recording stops automatically after you click the stop button</li>
      </ul>
    </div>
  </div>
  
  <!-- Prompt Section -->
  <div class="prompt-section">
    <h3>Interview Prompt</h3>
    <textarea id="interviewPrompt" rows="8" style="width: 100%;">{{ interview_prompt }}</textarea>
    <div style="margin-top: 10px;">
      <button id="startButton" class="primary">Start Interview</button>
      <button id="resetButton" class="danger" disabled>Reset Interview</button>
    </div>
  </div>
  
  <!-- Interview Section -->
  <div id="interviewSection" class="interview-section hidden">
    <h3>Interview In Progress</h3>
    
    <!-- Transcript Display -->
    <div id="transcriptArea" class="transcript-area"></div>
    
    <!-- Audio Controls -->
    <div class="audio-controls">
      <button id="playButton" class="secondary" disabled>
        <i class="fas fa-play"></i> Play Question
      </button>
      <button id="recordButton" class="secondary">
        <i class="fas fa-microphone"></i> Record Answer
      </button>
      <span id="recordingStatus"></span>
    </div>
    
    <!-- Response Controls -->
    <div class="response-area">
      <textarea id="responseInput" placeholder="Type your response here or use the microphone to record..."></textarea>
    </div>
    
    <!-- Action Buttons -->
    <div class="action-buttons">
      <button id="sendButton" class="primary" disabled>Send Response</button>
      <button id="analyzeButton" class="success" disabled>Analyze Interview</button>
    </div>
    
    <!-- Audio Element (hidden) -->
    <audio id="audioPlayer" style="display: none;"></audio>
  </div>
  
  <!-- Analysis Section -->
  <div id="analysisSection" class="analysis-section hidden">
    <h3>Interview Analysis</h3>
    <div id="analysisContent"></div>
  </div>
</div>

{% endblock %}

{% block scripts %}
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // DOM Elements
    const interviewPrompt = document.getElementById('interviewPrompt');
    const startButton = document.getElementById('startButton');
    const resetButton = document.getElementById('resetButton');
    const interviewSection = document.getElementById('interviewSection');
    const transcriptArea = document.getElementById('transcriptArea');
    const playButton = document.getElementById('playButton');
    const recordButton = document.getElementById('recordButton');
    const recordingStatus = document.getElementById('recordingStatus');
    const responseInput = document.getElementById('responseInput');
    const sendButton = document.getElementById('sendButton');
    const analyzeButton = document.getElementById('analyzeButton');
    const audioPlayer = document.getElementById('audioPlayer');
    const voiceSelect = document.getElementById('voiceSelect');
    const analysisSection = document.getElementById('analysisSection');
    const analysisContent = document.getElementById('analysisContent');
    
    // State variables
    let mediaRecorder;
    let audioChunks = [];
    let isRecording = false;
    let sessionActive = false;
    let lastQuestionText = '';
    
    // Event Listeners
    startButton.addEventListener('click', startInterview);
    resetButton.addEventListener('click', resetInterview);
    playButton.addEventListener('click', playCurrentQuestion);
    recordButton.addEventListener('click', toggleRecording);
    sendButton.addEventListener('click', sendResponse);
    analyzeButton.addEventListener('click', analyzeInterview);
    responseInput.addEventListener('input', checkResponseValid);
    document.getElementById('testMicButton').addEventListener('click', testMicrophone);
    
    // Function to start the interview
    function startInterview() {
      const prompt = interviewPrompt.value.trim();
      if (!prompt) {
        alert('Please enter an interview prompt');
        return;
      }
      
      // Disable start button and prompt area during API call
      startButton.disabled = true;
      interviewPrompt.disabled = true;
      
      // Call API to start interview
      fetch('/api/langchain_interview/start', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          interview_prompt: prompt
        })
      })
      .then(response => response.json())
      .then(data => {
        if (data.error) {
          alert('Error: ' + data.error);
          startButton.disabled = false;
          interviewPrompt.disabled = false;
          return;
        }
        
        // Show interview section
        interviewSection.classList.remove('hidden');
        resetButton.disabled = false;
        sessionActive = true;
        
        // Display first question
        const firstQuestion = data.first_question;
        lastQuestionText = firstQuestion;
        addMessageToTranscript('assistant', firstQuestion);
        
        // Enable play button
        playButton.disabled = false;
        convertTextToSpeech(firstQuestion);
      })
      .catch(error => {
        console.error('Error starting interview:', error);
        alert('Error starting interview. See console for details.');
        startButton.disabled = false;
        interviewPrompt.disabled = false;
      });
    }
    
    // Function to reset the interview
    function resetInterview() {
      // Reset UI
      startButton.disabled = false;
      interviewPrompt.disabled = false;
      resetButton.disabled = true;
      interviewSection.classList.add('hidden');
      transcriptArea.innerHTML = '';
      responseInput.value = '';
      sendButton.disabled = true;
      playButton.disabled = true;
      analyzeButton.disabled = true;
      analysisSection.classList.add('hidden');
      
      // Reset state
      sessionActive = false;
      lastQuestionText = '';
      
      // Stop any ongoing recording
      if (isRecording) {
        stopRecording();
      }
    }
    
    // Function to play the current question
    function playCurrentQuestion() {
      if (!lastQuestionText) return;
      
      // Convert text to speech and play
      convertTextToSpeech(lastQuestionText);
    }
    
    // Function to convert text to speech
    function convertTextToSpeech(text) {
      const voiceId = voiceSelect.value;
      
      // Show loading state
      playButton.disabled = true;
      playButton.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Loading...';
      
      fetch('/api/text_to_speech_elevenlabs', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          text: text,
          voice_id: voiceId
        })
      })
      .then(response => {
        if (!response.ok) {
          throw new Error('Text to speech failed');
        }
        return response.blob();
      })
      .then(audioBlob => {
        const audioUrl = URL.createObjectURL(audioBlob);
        audioPlayer.src = audioUrl;
        audioPlayer.onended = () => {
          URL.revokeObjectURL(audioUrl);
        };
        audioPlayer.play();
        
        // Reset button
        playButton.disabled = false;
        playButton.innerHTML = '<i class="fas fa-play"></i> Play Question';
      })
      .catch(error => {
        console.error('Error with text to speech:', error);
        alert('Error with text to speech. See console for details.');
        
        // Reset button
        playButton.disabled = false;
        playButton.innerHTML = '<i class="fas fa-play"></i> Play Question';
      });
    }
    
    // Function to toggle recording
    function toggleRecording() {
      if (isRecording) {
        stopRecording();
      } else {
        startRecording();
      }
    }
    
    // Function to start recording
    function startRecording() {
      // Request microphone permission
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          mediaRecorder = new MediaRecorder(stream);
          audioChunks = [];
          
          mediaRecorder.ondataavailable = event => {
            audioChunks.push(event.data);
          };
          
          mediaRecorder.onstop = () => {
            const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
            
            // Show transcribing status
            responseInput.value = "Transcribing...";
            
            // Create form data to send to server
            const formData = new FormData();
            formData.append('audio', audioBlob);
            formData.append('project_name', 'LangchainTest');
            
            // Send to server for transcription using the process_audio endpoint
            fetch('/process_audio?project_name=LangchainTest', {
              method: 'POST',
              body: formData
            })
            .then(response => response.json())
            .then(data => {
              if (data.error) {
                console.error('Transcription error:', data.error);
                responseInput.value = "Error transcribing speech. Please type your response.";
                responseInput.select();
                return;
              }
              
              // Success - set the transcription in the input field
              responseInput.value = data.transcription || data.transcript || "";
              checkResponseValid();
            })
            .catch(error => {
              console.error('Error sending audio for transcription:', error);
              responseInput.value = "Error transcribing speech. Please type your response.";
              responseInput.select();
            });
          };
          
          // Start recording with small time slices for better streaming
          mediaRecorder.start(100);
          isRecording = true;
          
          // Update UI
          recordButton.innerHTML = '<i class="fas fa-stop"></i> Stop Recording';
          recordButton.classList.add('danger');
          recordingStatus.textContent = "Recording...";
          recordingStatus.classList.add('recording');
        })
        .catch(error => {
          console.error('Error accessing microphone:', error);
          alert('Error accessing microphone. Please check your browser permissions.');
        });
    }
    
    // Function to stop recording
    function stopRecording() {
      if (mediaRecorder && isRecording) {
        mediaRecorder.stop();
        isRecording = false;
        
        // Stop all tracks in the stream to release the microphone
        if (mediaRecorder.stream) {
          mediaRecorder.stream.getTracks().forEach(track => track.stop());
        }
        
        // Update UI
        recordButton.innerHTML = '<i class="fas fa-microphone"></i> Record Answer';
        recordButton.classList.remove('danger');
        recordingStatus.textContent = "";
        recordingStatus.classList.remove('recording');
        
        // Enable send button after transcription completes
        // (sendButton will be enabled by the transcription callback)
      }
    }
    
    // Function to check if response is valid
    function checkResponseValid() {
      sendButton.disabled = responseInput.value.trim() === '';
    }
    
    // Function to send response
    function sendResponse() {
      const response = responseInput.value.trim();
      if (!response) return;
      
      // Disable UI during API call
      sendButton.disabled = true;
      responseInput.disabled = true;
      
      // Add user message to transcript
      addMessageToTranscript('user', response);
      
      // Call API to get next question
      fetch('/api/langchain_interview/respond', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          user_input: response
        })
      })
      .then(response => response.json())
      .then(data => {
        if (data.error) {
          alert('Error: ' + data.error);
          sendButton.disabled = false;
          responseInput.disabled = false;
          return;
        }
        
        // Display next question
        const nextQuestion = data.next_question;
        lastQuestionText = nextQuestion;
        addMessageToTranscript('assistant', nextQuestion);
        
        // Reset response input
        responseInput.value = '';
        responseInput.disabled = false;
        
        // Play the next question
        convertTextToSpeech(nextQuestion);
        
        // Enable analyze button after a few exchanges (3+ user responses)
        const userMessages = transcriptArea.querySelectorAll('.message.user');
        if (userMessages.length >= 3) {
          analyzeButton.disabled = false;
        }
      })
      .catch(error => {
        console.error('Error sending response:', error);
        alert('Error sending response. See console for details.');
        sendButton.disabled = false;
        responseInput.disabled = false;
      });
    }
    
    // Function to add message to transcript
    function addMessageToTranscript(role, content) {
      const messageEl = document.createElement('div');
      messageEl.className = `message ${role}`;
      
      const nameSpan = document.createElement('strong');
      nameSpan.textContent = role === 'assistant' ? 'Daria: ' : 'You: ';
      
      const contentSpan = document.createElement('span');
      contentSpan.textContent = content;
      
      messageEl.appendChild(nameSpan);
      messageEl.appendChild(contentSpan);
      
      transcriptArea.appendChild(messageEl);
      
      // Scroll to bottom
      transcriptArea.scrollTop = transcriptArea.scrollHeight;
    }
    
    // Function to analyze the interview
    function analyzeInterview() {
      // Disable button during API call
      analyzeButton.disabled = true;
      analyzeButton.textContent = 'Analyzing...';
      
      // Call API to analyze interview
      fetch('/api/langchain_interview/analyze', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        }
      })
      .then(response => response.json())
      .then(data => {
        if (data.error) {
          alert('Error: ' + data.error);
          analyzeButton.disabled = false;
          analyzeButton.textContent = 'Analyze Interview';
          return;
        }
        
        // Display analysis
        analysisSection.classList.remove('hidden');
        analysisContent.textContent = data.analysis;
        
        // Re-enable button
        analyzeButton.disabled = false;
        analyzeButton.textContent = 'Analyze Interview';
      })
      .catch(error => {
        console.error('Error analyzing interview:', error);
        alert('Error analyzing interview. See console for details.');
        analyzeButton.disabled = false;
        analyzeButton.textContent = 'Analyze Interview';
      });
    }
    
    // Function to test microphone
    function testMicrophone() {
      const testButton = document.getElementById('testMicButton');
      const resultSpan = document.getElementById('micTestResult');
      
      // Update UI
      testButton.disabled = true;
      testButton.innerHTML = '<i class="fas fa-spinner fa-spin"></i> Testing...';
      resultSpan.textContent = '';
      
      // Request microphone permission
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          // Create recorder
          const testRecorder = new MediaRecorder(stream);
          const testChunks = [];
          
          testRecorder.ondataavailable = event => {
            testChunks.push(event.data);
          };
          
          testRecorder.onstop = () => {
            const audioBlob = new Blob(testChunks, { type: 'audio/wav' });
            
            // Send to diagnostic endpoint
            const formData = new FormData();
            formData.append('audio', audioBlob);
            
            fetch('/api/diagnostics/microphone', {
              method: 'POST',
              body: formData
            })
            .then(response => response.json())
            .then(data => {
              // Reset button
              testButton.disabled = false;
              testButton.innerHTML = '<i class="fas fa-microphone"></i> Test Microphone';
              
              if (data.error) {
                resultSpan.textContent = '‚ùå Error: ' + data.error;
                resultSpan.style.color = 'red';
                return;
              }
              
              if (data.status === 'success') {
                resultSpan.textContent = '‚úÖ Microphone working!';
                resultSpan.style.color = 'green';
                if (data.transcription) {
                  resultSpan.textContent += ' Heard: "' + data.transcription + '"';
                }
              } else {
                resultSpan.textContent = '‚ùå Test failed. Try again.';
                resultSpan.style.color = 'red';
              }
            })
            .catch(error => {
              console.error('Error testing microphone:', error);
              testButton.disabled = false;
              testButton.innerHTML = '<i class="fas fa-microphone"></i> Test Microphone';
              resultSpan.textContent = '‚ùå Connection error. Try again.';
              resultSpan.style.color = 'red';
            });
          };
          
          // Record for 3 seconds then stop
          testRecorder.start();
          resultSpan.textContent = 'üéôÔ∏è Please speak now...';
          resultSpan.style.color = 'blue';
          
          setTimeout(() => {
            testRecorder.stop();
            stream.getTracks().forEach(track => track.stop());
          }, 3000);
        })
        .catch(error => {
          console.error('Error accessing microphone:', error);
          testButton.disabled = false;
          testButton.innerHTML = '<i class="fas fa-microphone"></i> Test Microphone';
          resultSpan.textContent = '‚ùå Could not access microphone. Check browser permissions.';
          resultSpan.style.color = 'red';
        });
    }
  });
</script>
{% endblock %} 